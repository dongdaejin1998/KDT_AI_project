{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOilWfkp6bwuNpBv+KwQWIz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4voKTnxsyY7J"},"outputs":[],"source":["# 깃허브에서 데이터셋 다운로드하기\n","!git clone https://github.com/ndb796/Scene-Classification-Dataset\n","# 폴더 안으로 이동\n","%cd Scene-Classification-Dataset"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","\n","path = 'train-scene classification/'\n","\n","# 전체 이미지 개수 출력하기\n","file_list = os.listdir(path + 'train/')\n","print('전체 이미지의 개수:', len(file_list))\n","\n","# 학습 이미지 확인하기\n","data = pd.read_csv(path + 'train.csv')\n","print('학습 이미지의 개수:', len(data))\n","print('학습 이미지별 클래스 정보')\n","data.head()"],"metadata":{"id":"ZrI3Japeraw-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_val=train_test_split(data,test_size=0.2,random_state=42)"],"metadata":{"id":"pvHdAWwhresw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_val,X_test=train_test_split(X_val,test_size=0.5,random_state=42)"],"metadata":{"id":"z1zfwaWerj2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset\n","from skimage import io, color\n","import pandas as pd\n","import os\n","from tqdm.notebook import tqdm"],"metadata":{"id":"SF5FtY8hynTc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset,DataLoader\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, labels, img_dir, transform=None, target_transform=None):\n","        self.img_labels = labels\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return [image, label]"],"metadata":{"id":"RLljU03tykqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.AugMix(),\n","    transforms.RandAugment(),\n","    transforms.ToTensor(), \n","#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # 이미지 정규화\n","])\n","\n","test_transform=transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.ToTensor(), \n","])"],"metadata":{"id":"dvQGbtlYrRo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = CustomImageDataset(X_train,\"/content/Scene-Classification-Dataset/train-scene classification/train\",transform=transform )\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"pL2P8LSkq0m5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_dataset = CustomImageDataset(X_val,\"/content/Scene-Classification-Dataset/train-scene classification/train\",transform=test_transform )\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"ptp8dqDarJ15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = CustomImageDataset(X_test,\"/content/Scene-Classification-Dataset/train-scene classification/train\" ,transform=test_transform )\n","test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"],"metadata":{"id":"mxPApc6O3IIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import resnet\n","import torchvision.models.resnet as resnet\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# 미리 정의\n","conv1x1=resnet.conv1x1\n","Bottleneck = resnet.Bottleneck\n","BasicBlock= resnet.BasicBlock"],"metadata":{"id":"OaFcCMwmwYgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNet(nn.Module):\n","\n","    def __init__(self, block, layers, num_classes=1000, zero_init_residual=True):\n","        super(ResNet, self).__init__()\n","        self.inplanes = 32 # conv1에서 나올 채널의 차원 -> 이미지넷보다 작은 데이터이므로 32로 조정\n","\n","        # inputs = 3x224x224 -> 3x128x128로 바뀜\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False) # 마찬가지로 전부 사이즈 조정\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        \n","        self.layer1 = self._make_layer(block, 32, layers[0], stride=1) # 3 반복\n","        self.layer2 = self._make_layer(block, 64, layers[1], stride=2) # 4 반복\n","        self.layer3 = self._make_layer(block, 128, layers[2], stride=2) # 6 반복\n","        self.layer4 = self._make_layer(block, 256, layers[3], stride=2) # 3 반복\n","        \n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(256 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck):\n","                    nn.init.constant_(m.bn3.weight, 0)\n","                elif isinstance(m, BasicBlock):\n","                    nn.init.constant_(m.bn2.weight, 0)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1): # planes -> 입력되는 채널 수\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion: \n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                nn.BatchNorm2d(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        # input [32, 128, 128] -> [C ,H, W]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","        #x.shape =[32, 64, 64]\n","\n","        x = self.layer1(x)\n","        #x.shape =[128, 64, 64]\n","        x = self.layer2(x)\n","        #x.shape =[256, 32, 32]\n","        x = self.layer3(x)\n","        #x.shape =[512, 16, 16]\n","        x = self.layer4(x)\n","        #x.shape =[1024, 8, 8]\n","        \n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","\n","        return x"],"metadata":{"id":"WfA-2GEKxb5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"],"metadata":{"id":"OUBkvsmarwZ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 6, True).to(device)"],"metadata":{"id":"449eXv9ixh5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_accuracy(loader, model):\n","    num_correct = 0\n","    num_samples = 0\n","    # Don't forget to toggle to eval mode!\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        for data, targets in tqdm(loader):\n","            data = data.to(device)\n","            targets = targets.to(device)\n","            scores = model(data)\n","            _, predictions = scores.max(1)\n","            num_correct += (predictions == targets).sum()\n","            num_samples += predictions.size(0)\n","        print(\"Correct: {}, Total: {}, Accuracy: {}\".format(num_correct, num_samples, int(num_correct) / int(num_samples)))\n","    # Don't forget to toggle back to model.train() since you're done with evaluation\n","    model.train()"],"metadata":{"id":"WyGKRDkpzYWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    LEARNING_RATE = 0.0001\n","    # You could try playing around with the batch size(say 16) and learning rate(say 0.001) for faster convergence.\n","    BATCH_SIZE = 64\n","    EPOCHS = 10\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    transform_img = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor()\n","    ])\n","\n","    \n","\n","    \n","    model = model\n","    #model.to(device)\n","    # change the output layer to 10 classes\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n","    torch.cuda.empty_cache()   \n","    data, targets = next(iter(train_dataloader))\n","    for epoch in tqdm(range(EPOCHS)):\n","        losses = []\n","        with tqdm(total=len(train_dataloader)) as pbar:\n","            for batch_idx, (data, targets) in enumerate(train_dataloader):\n","                data = data.to(device=device)\n","                targets = targets.to(device=device)\n","\n","                # backprop\n","                optimizer.zero_grad()\n","\n","\n","                scores = model(data)\n","                loss = criterion(scores, targets)\n","                losses.append(loss)\n","\n","                \n","                loss.backward()\n","                optimizer.step()\n","#                 print(loss.item())\n","                pbar.update(1)\n","        print(\"Cost at epoch {} is {}\".format(epoch, sum(losses) / len(losses)))\n","        check_accuracy(train_dataloader, model)\n","        check_accuracy(val_dataloader, model)"],"metadata":{"id":"jwgTsnzbyrVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 깃허브에서 데이터셋 다운로드하기\n","!git clone https://github.com/ndb796/Scene-Classification-Dataset\n","# 폴더 안으로 이동\n","%cd Scene-Classification-Dataset"],"metadata":{"id":"hiJuSk8wevK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","\n","path = 'train-scene classification/'\n","\n","# 전체 이미지 개수 출력하기\n","file_list = os.listdir(path + 'train/')\n","print('전체 이미지의 개수:', len(file_list))\n","\n","# 학습 이미지 확인하기\n","data = pd.read_csv(path + 'train.csv')\n","print('학습 이미지의 개수:', len(data))\n","print('학습 이미지별 클래스 정보')\n","data.head()"],"metadata":{"id":"Ysc4TVjCe68u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_val=train_test_split(data,test_size=0.2,random_state=42)"],"metadata":{"id":"E8JFXF0GfAba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_val,X_test=train_test_split(X_val,test_size=0.5,random_state=42)"],"metadata":{"id":"WWg0jJuafBvq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset,DataLoader\n","\n","class CustomImageDataset(Dataset):\n","    def __init__(self, labels, img_dir, transform=None, target_transform=None):\n","        self.img_labels = labels\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image.float(), label"],"metadata":{"id":"c9a1Uw--fD3I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.AugMix(),\n","    transforms.RandAugment(),\n","    transforms.ToTensor(), \n","#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), # 이미지 정규화\n","])\n","\n","test_transform=transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(size=(224,224)),\n","    transforms.ToTensor(), \n","])"],"metadata":{"id":"ooJ2esY3fF4-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_dataset = CustomImageDataset(X_train,\"/content/Scene-Classification-Dataset/train-scene classification/train\",transform=transform )\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"I7KZwk8jfHpL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_dataset = CustomImageDataset(X_val,\"/content/Scene-Classification-Dataset/train-scene classification/train\",transform=test_transform )\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"9WsWrLkpfJrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = CustomImageDataset(X_test,\"/content/Scene-Classification-Dataset/train-scene classification/train\" ,transform=test_transform )\n","test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=True)"],"metadata":{"id":"lcs-QjoxfKLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","\n","\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")"],"metadata":{"id":"YpWkybyOfP1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","# Hyper-parameters\n","num_epochs = 10\n","learning_rate = 0.001\n"],"metadata":{"id":"ZnnDZEFRf6pE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset\n","from skimage import io, color\n","import pandas as pd\n","import os\n","from tqdm.notebook import tqdm"],"metadata":{"id":"QSvvb6P2n07z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Takes inputs with dims = (N, C, *)\n","# Gives outputs with dimes = (N, C, *)\n","class LocalResponseNormalization(nn.Module):\n","    def __init__(self, neighbourhood_length, normalisation_const_alpha, contrast_const_beta, noise_k):\n","        super(LocalResponseNormalization, self).__init__()\n","        self.nbd_len = neighbourhood_length\n","        self.alpha = normalisation_const_alpha\n","        self.beta = contrast_const_beta\n","        self.k = noise_k\n","    \n","    # The following is exactly what pytorch does under the hood as well. I only replicated it for my understanding :)\n","    def forward(self, x):\n","        # Lets validate if x is atleast 3 dimensional\n","        dim = x.dim()\n","        if dim < 3:\n","            raise ValueError(\"Expected tensor of atleast 3 dimensions, found only {}\".format(dim))\n","        denom = x.pow(2).unsqueeze(1)\n","        if dim == 3:\n","            denom = F.pad(denom, (0, 0, self.nbd_len // 2, (self.nbd_len - 1) // 2))\n","            denom = F.avg_pool2d(denom, (self.nbd_len, 1), stride=1)\n","            denom = denom.squeeze(1)\n","        else:\n","            sizes = x.size()\n","            # The last two dimensions make up a single channel. The third dimension decides the number of channels\n","            # across which we will apply local response normalization.\n","            denom = denom.view(sizes[0], 1, sizes[1], sizes[2], -1)\n","            # The point is to pad in front and back of the channels across which we'll apply normalization\n","            denom = F.pad(denom, (0, 0, 0, 0, self.nbd_len // 2, (self.nbd_len - 1) // 2))\n","            denom = F.avg_pool3d(denom, (self.nbd_len, 1, 1), stride=1)\n","            denom = denom.squeeze(1).view(sizes)\n","        denom = denom.mul(self.alpha).add(self.k).pow(self.beta)\n","        return x.div(denom)"],"metadata":{"id":"W7XvUnQgn5R-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Expects input tensor to be of dimensions (batch_size, 3, 224, 224)\n","class Alexnet(nn.Module):\n","    def __init__(self):\n","        super(Alexnet, self).__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n","        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n","        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n","        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n","        self.fc1 = nn.Linear(in_features=256 * 6 * 6, out_features=4096)\n","        self.fc2 = nn.Linear(in_features=4096, out_features=4096)\n","        self.fc3 = nn.Linear(in_features=4096, out_features=6)\n","        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2)\n","        # This layer helps us avoid calculating output map size when feeding into a linear layer in PyTorch.\n","        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(6, 6))\n","        self.norm = LocalResponseNormalization(neighbourhood_length=5, normalisation_const_alpha=1e-4, contrast_const_beta=0.75, noise_k=1.0)\n","        self.dropout = nn.Dropout()\n","    \n","    def forward(self, x):\n","        x = self.max_pool(self.norm(F.relu(self.conv1(x))))\n","        x = self.max_pool(self.norm(F.relu(self.conv2(x))))\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = self.adaptive_pool(self.norm(F.relu(self.conv5(x))))\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"Fe69gSUPn8dv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Alexnet().to(device)"],"metadata":{"id":"j06ilnG0oDB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"],"metadata":{"id":"JSoOHx6tgeF9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, criterion,train,val, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    \n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # 각 에폭(epoch)은 학습 단계와 검증 단계를 갖습니다.\n","        model.train()  # 모델을 학습 모드로 설정\n","                \n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","        test_loss=0.0\n","        test_corrects=0\n","        # 데이터를 반복\n","        for inputs, labels in train:\n","            #print(inputs.size())\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","                # 매개변수 경사도를 0으로 설정\n","            optimizer.zero_grad()\n","\n","                # 순전파\n","                # 학습 시에만 연산 기록을 추적\n","            \n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","                # 학습 단계인 경우 역전파 + 최적화\n","                    \n","            loss.backward()\n","            optimizer.step()\n","\n","            # 통계\n","            running_loss += loss.item() \n","            running_corrects += torch.sum(preds == labels.data)/len(labels)\n","            #print(len(labels),torch.sum(preds == labels.data))\n","            #print(running_loss,running_corrects)\n","           \n","            scheduler.step()\n","        #print(len(train))\n","        epoch_loss = running_loss / len(train)\n","        epoch_acc = running_corrects.double() / len(train)\n","\n","        # 모델을 평가 모드로 설정\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                \"train\", epoch_loss, epoch_acc*100))\n","        model.eval()\n","\n","        for inputs, labels in val:\n","   \n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","\n","                # 순전파\n","                # 학습 시에만 연산 기록을 추적\n","            \n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","            test_loss += loss.item()\n","            test_corrects += torch.sum(preds == labels.data)/len(labels)\n","        epoch_loss = test_loss / len(val)\n","        epoch_acc = test_corrects.double() /len(val)\n","\n","           # 모델을 평가 모드로 설정\n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                \"Val\", epoch_loss, epoch_acc*100))\n","            # 모델을 깊은 복사(deep copy)함\n","        if epoch_acc > best_acc:\n","            best_acc = epoch_acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # 가장 나은 모델 가중치를 불러옴\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"v12EpQyWgRGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_fit = train_model(model, criterion, train_dataloader,val_dataloader, optimizer, exp_lr_scheduler,\n","                       num_epochs)"],"metadata":{"id":"tA6Q4oLigXoK"},"execution_count":null,"outputs":[]}]}